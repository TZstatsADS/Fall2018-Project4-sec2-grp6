{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"detection_and_correction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"vvMwjQKCzwwe","colab_type":"code","colab":{}},"cell_type":"code","source":["### Run this cell ONLY for Colab users ###\n","!unzip data.zip\n","# Otherwise, put the /data folder (containing two subfolders of 100 .txt files)\n","# under the working directory.\n","# Now we should have /data under the working directory."],"execution_count":0,"outputs":[]},{"metadata":{"id":"InSqRbZ2B0le","colab_type":"code","colab":{}},"cell_type":"code","source":["# Upload correction_lib.py and Create_Words_Dictionary.py to Colab,\n","# or make sure they are in the same directory of this file (for local machine).\n","# Now, import correction_lib and Create_Words_Dictionary modules. \n","import correction_lib as corr\n","from word_dict import Create_Words_Dictionary, Create_Word_Pair"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GJnaq0M11L-f","colab_type":"code","colab":{}},"cell_type":"code","source":["import string\n","import glob\n","import os\n","import itertools\n","import collections\n","import timeit\n","import random\n","import pandas as pd\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dwH9Qqpo3n-7","colab_type":"code","colab":{}},"cell_type":"code","source":["def clean_word(w):\n","    out = []\n","    for c in w:\n","        c = c.lower()\n","        # Searching set is faster than list: O(1) vs. O(n=26)\n","        if c in set(string.ascii_lowercase):\n","            out.append(c)\n","    return(''.join(out))\n","# clean_word('Caat13.#abE')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TZRT73_9-z1T","colab_type":"code","colab":{}},"cell_type":"code","source":["def char_to_index(c):\n","    return(ord(c) - ord('a'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wrIMMKkR_-Dr","colab_type":"code","colab":{}},"cell_type":"code","source":["# print matrices/digrams in a clear manner \n","def print_matrix(matrix):\n","    alphabet = ' ' + string.ascii_lowercase\n","    print('  '.join(alphabet))\n","    for i in range(len(matrix)):\n","        print(chr(ord('a')+i), matrix[i])\n","# print_digram(digrams_by_len[3][(0, 1)])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SFC8_z10QWXG","colab_type":"text"},"cell_type":"markdown","source":["#Error Detection"]},{"metadata":{"id":"N5pRaUigcZJQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a list of words from ground truth; include repeatition and order of words\n","word_list = []\n","gt_filenames = glob.glob(os.path.join(os.getcwd(), 'data', 'ground_truth', '*.txt'))\n","\n","count = 0\n","for gt_f in gt_filenames:\n","    with open(gt_f) as file:\n","        raw = file.read()\n","        # Split file content into words (by '\\n', '\\t', ' ', etc.)\n","        uncleaned_words = raw.split()\n","        # Clean up words, leave only all-alpha chars of length > 1 (function programming)\n","        word_list += list(filter(lambda x: 1 < len(x) < 21, map(clean_word, uncleaned_words)))\n","# print(len(word_list))\n","# print(word_list[:20])\n","# word_set = set(word_list)\n","# print(len(word_set))\n","# print(list(word_set)[:20])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TdZW-zdo6RQo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Categorize ground truth words by their length\n","group_by_len = collections.defaultdict(list)\n","for w in word_set:\n","    group_by_len[len(w)].append(w)\n","\n","# A dictionary of positional binary digrams (matrices),\n","# ordered by word length and then by binary positions\n","digrams_by_len = collections.defaultdict(dict)\n","for length in group_by_len:\n","    for i, j in itertools.combinations(range(length), 2):\n","        key = (i, j)\n","        matrix = [[0] * 26 for _ in range(26)]\n","        for w in group_by_len[length]:\n","            matrix[char_to_index(w[i])][char_to_index(w[j])] = 1\n","        digrams_by_len[length][key] = matrix\n","#     print(length, len(digrams_by_len[length].keys()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ufAOiMFfCCCe","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create a list of words from tesseract text; regard repeatition and order of words\n","tr_word_list = []\n","tr_filenames = glob.glob(os.path.join(os.getcwd(), 'data', 'tesseract', '*.txt'))\n","for tr_f in tr_filenames:\n","    with open(tr_f) as file:\n","        raw = file.read()\n","        uncleaned_words = raw.split()\n","        tr_word_list += list(filter(lambda x: 1 < len(x) < 21, map(clean_word, uncleaned_words)))\n","N_tr = len(tr_word_list)\n","# print(N_tr, '\\n', tr_word_list[:30])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"74nJtg6nMF30","colab_type":"code","colab":{}},"cell_type":"code","source":["# A list of 3-tuples, each consisting of (detected error, left word, right word)\n","detected_error_tuples = []\n","for idx, w in enumerate(tr_word_list):\n","    error = False\n","    for i, j in itertools.combinations(range(len(w)), 2):\n","#         print(i, j, w[i], w[j], len(digrams_by_len[len(w)][(i, j)]), len(digrams_by_len[len(w)][(i, j)][0]))\n","        if not digrams_by_len[len(w)][(i, j)][char_to_index(w[i])][char_to_index(w[j])]:\n","            error = True\n","    if error:\n","        left = tr_word_list[i-1] if i > 0 else ''\n","        right = tr_word_list[i+1] if i < len(tr_word_list)-1 else ''\n","        detected_error_tuples.append((w, left, right))\n","\n","# A list of detected error words\n","detected_error_words = [x[0] for x in detected_error_tuples]\n","        \n","# print(len(detected_error_tuples), len(detected_error_words))        \n","# print(detected_error_tuples[:10])\n","# print(detected_error_words[:10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"epnb2romQeX1","colab_type":"text"},"cell_type":"markdown","source":["#Error Correction"]},{"metadata":{"id":"5FB5VHa6BeGS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create confusion matrices\n","Confusion = corr.Create_Confusion_Matrix()\n","# print_matrix(Confusion['Deletion_Confusion'])\n","# print_matrix(Confusion['Insertion_Confusion'])\n","# print_matrix(Confusion['Substitution_Confusion'])\n","# print_matrix(Confusion['Reversal_Confusion'])\n","\n","# Useful values from section 3 of paper C-4.\n","N = len(word_list)\n","V = len(word_set)\n","denominator = N + V/2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"45vgGoIRP1ca","colab_type":"code","colab":{}},"cell_type":"code","source":["# Input a detected error word;\n","# return a list of (correction candidates, changed letters),\n","# ordered by types of correction (from the ground truth word set)\n","def get_correction_candidates(w):\n","    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n","    candidate_list = [[] for _ in range(4)]\n","    \n","    ### 4 kinds of correction candidates (see Table 2, C-4)\n","    # 0. Deletion\n","    for i in range(len(w) + 1):\n","        for c in alphabet:\n","            correction = w[:i] + c + w[i:]\n","            if correction in word_set:\n","                diff = corr.find_deletion_letters(correction, w) \n","                candidate_list[0].append((correction, diff['pre_letter'], diff['delete_letter']))\n","            \n","    # 1. Insertion\n","    for i in range(len(w)):\n","        correction = w[:i] + w[i+1:]\n","        if correction in word_set:\n","            diff = corr.find_insertion_letters(correction, w) \n","            candidate_list[1].append((correction, diff['pre_letter'], diff['insert_letter']))\n","    \n","    # 2. Substitution\n","    for i in range(len(w)):\n","        for c in alphabet:\n","            if c != w[i]:\n","                correction = w[:i] + c + w[i+1:]\n","                if correction in word_set:\n","                    diff = corr.find_sub_rev_letters(correction, w) \n","                    if diff['tag'] == 'sub':\n","                        candidate_list[2].append((correction, diff['pre_letter'], diff['changed_letter']))\n","    \n","    # 3. Reversal\n","    for i in range(len(w) - 1):\n","        correction = w[:i] + w[i+1] + w[i] + w[i+2:]\n","        if correction in word_set:\n","            diff = corr.find_sub_rev_letters(correction, w) \n","            if diff['tag'] == 'rev':\n","                candidate_list[3].append((correction, diff['pre_letter'], diff['changed_letter']))\n","    \n","    return(candidate_list)\n","# get_correction_candidates('en')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lXZnMlRYHyxB","colab_type":"code","outputId":"abcd9ed1-226b-4c18-b6e5-e87a00523e4c","executionInfo":{"status":"ok","timestamp":1543715547839,"user_tz":300,"elapsed":363529,"user":{"displayName":"Yaoqi Guo","photoUrl":"https://lh5.googleusercontent.com/-V0xosfnxPkc/AAAAAAAAAAI/AAAAAAAAAA4/W_djzIcpnsU/s64/photo.jpg","userId":"12895129010068449331"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# This cell needs approximately 90-120 seconds to run on Colab.\n","# Create a dictionary of all candidates of each detected error\n","start = timeit.default_timer()\n","\n","all_candidates = collections.defaultdict(dict)\n","for word in detected_error_words:\n","    all_candidates[word] = get_correction_candidates(word)\n","    \n","stop = timeit.default_timer()\n","print('Time:', stop - start, 'seconds')\n","\n","# print(len(detected_error_words))\n","# print(len(all_candidates))\n","# print(get_correction_candidates('hlth'))"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Time: 11.063699515000053 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"CdGJTtHVUQTJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# A dictionary of frequecies of words in the ground truth\n","word_freqs = collections.defaultdict(int)\n","for word in word_list:\n","    word_freqs[word] += 1\n","# print(dict((k, v) for k, v in word_freqs.items() if v >= 2))\n","\n","# Pr(c) of all possible corrections (all words from ground truth)\n","corr_probs = collections.defaultdict(float)\n","for word, freq in word_freqs.items():\n","    corr_probs[word] = (freq + 0.5)/denominator\n","# print(corr_probs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PicoZXM9IQiW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compute Pr(c), estimated by ELE (expected LE)\n","def get_Pr_c(correction):\n","    return(corr_probs[correction])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nQ64XaiBIEcA","colab_type":"code","colab":{}},"cell_type":"code","source":["# chars[x] and chars[xy]\n","chars_x = [0] * 26 + [N]\n","chars_xy = [[0] * 26 for _ in range(27)]\n","for word in word_list:\n","    for i, c in enumerate(word):\n","        chars_x[char_to_index(c)] += 1\n","        if i:\n","            chars_xy[26][char_to_index(c)] += 1\n","        else:\n","            chars_xy[char_to_index(word[i-1])][char_to_index(c)] += 1\n","for i in range(len(chars_xy)):\n","    for j in range(len(chars_xy[0])):\n","        if not chars_xy[i][j]:\n","            chars_xy[i][j] = 0.5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bSmT12BQKcvB","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_Pr_tc(typo, correction, pre, cur, error_type):\n","    if error_type == 'del':\n","        return Confusion['Deletion_Confusion'][char_to_index(pre)][char_to_index(cur)] \\\n","               / chars_xy[char_to_index(pre)][char_to_index(cur)]\n","    if error_type == 'ins':\n","        return Confusion['Insertion_Confusion'][char_to_index(pre)][char_to_index(cur)] \\\n","               / chars_x[char_to_index(pre)]\n","    if error_type == 'sub':\n","        return Confusion['Substitution_Confusion'][char_to_index(pre)][char_to_index(cur)] \\\n","               / chars_x[char_to_index(pre)]\n","    if error_type == 'rev':\n","        return Confusion['Reversal_Confusion'][char_to_index(pre)][char_to_index(cur)] \\\n","               / chars_xy[char_to_index(pre)][char_to_index(cur)]\n","# print(get_Pr_tc('ncreased', 'increased', '{', 'i', 'del'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5FFfTz_LnUsn","colab_type":"code","colab":{}},"cell_type":"code","source":["# A dictionary, the key of which is the words in ground truth,\n","# the value to each key records the two neighbor words and their frequencies.\n","neighbor_dict = Create_Words_Dictionary()\n","# neighbor_dict['reliable']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HdXXqlsTt9Rm","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_Pr_context_correction(cand, left, right, method):\n","    \"\"\"\n","    word:   a correction candidate\n","    left:   the left neighbor of the error word in tesseract\n","    right:  the right neighbor of the error word in tesseract\n","    method: 'MLE', or 'ELE' where r = freq + 0.5\n","    return: ELE of Pr(l|c) * Pr(r|c)\n","    \"\"\"\n","    # r_left = freq of left appearing, r_right = freq of right appearing\n","    r_left, r_right = 0, 0\n","    if left in neighbor_dict[cand]['left']:\n","        r_left = neighbor_dict[cand]['left'][left]\n","    if right in neighbor_dict[cand]['right']:\n","        r_right = neighbor_dict[cand]['right'][right]\n","        \n","    if method == 'MLE':\n","        return(r_left*r_right)\n","    else: # method == 'ELE'\n","        return((r_left + 0.5)*(r_right + 0.5))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NOz3K35gwgI3","colab_type":"code","colab":{}},"cell_type":"code","source":["result = Create_Word_Pair()\n","word_pairs = result[:-1]\n","num_corrections_found_tesseract, sum_letters = result[-1][0], result[-1][1]\n","# print(sum_letters)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PJ2e2EI1nWzh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e994bb42-0b9e-451e-b5c3-20c0b4c07cfd","executionInfo":{"status":"ok","timestamp":1543715675253,"user_tz":300,"elapsed":490796,"user":{"displayName":"Yaoqi Guo","photoUrl":"https://lh5.googleusercontent.com/-V0xosfnxPkc/AAAAAAAAAAI/AAAAAAAAAA4/W_djzIcpnsU/s64/photo.jpg","userId":"12895129010068449331"}}},"cell_type":"code","source":["# This cell needs approximately 90-120 seconds to run on Colab.\n","start = timeit.default_timer()\n","detected_error_tuples2 = []\n","\n","for t in detected_error_tuples:\n","    for pair in word_pairs:\n","        if t[0] == pair[1]:\n","            detected_error_tuples2.append((t[0], pair[0], t[1], t[2]))\n","            break\n","stop = timeit.default_timer() \n","print('Time:', stop - start, 'seconds')\n","# print(len(detected_error_tuples2), detected_error_tuples2[:20])"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Time: 122.72246861899998 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"cbsV9VOBnmle","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_final_Pr_correction(cand, typo, pre, cur, error_type, left, right, method):\n","    return get_Pr_c(cand) * get_Pr_tc(typo, cand, pre, cur, error_type) \\\n","           * get_Pr_context_correction(cand, left, right, method)\n","\n","err_type = ['del', 'ins', 'sub', 'rev']\n","# A dictionary of the form: typo:(ground_truth, MLE_correction, ELE_correction)\n","result = {}\n","for typo, truth, left, right in detected_error_tuples2:\n","    cand_poss_mle, cand_poss_ele = [], []\n","    for e in range(4):\n","        for cand, pre, cur in all_candidates[typo][e]:\n","            mle_p = get_final_Pr_correction(cand, typo, pre, cur, err_type[e], left, right, 'MLE')\n","            ele_p = get_final_Pr_correction(cand, typo, pre, cur, err_type[e], left, right, 'ELE')\n","            cand_poss_mle.append((cand, mle_p))\n","            cand_poss_ele.append((cand, ele_p))\n","    mle_best_cand = max(cand_poss_mle, key=lambda x: x[1])[0] if cand_poss_mle else ''\n","    ele_best_cand = max(cand_poss_ele, key=lambda x: x[1])[0] if cand_poss_ele else ''\n","    result[typo] = (truth, mle_best_cand, ele_best_cand)\n","\n","# print(len(result))\n","# print(dict((k, v) for k, v in result.items() if 2 < len(k) < 5))\n","# print(dict((k, v) for k, v in result.items() if v[0] == v[1]))\n","# print(len(dict((k, v) for k, v in result.items() if v[0] == v[1])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B7lyd8fndUiF","colab_type":"text"},"cell_type":"markdown","source":["# Performance Measure"]},{"metadata":{"id":"Ac-HJtoOrHUC","colab_type":"code","colab":{}},"cell_type":"code","source":["# Word-wise recalls and precisions\n","num_corrections_found_MLE = num_corrections_found_tesseract\\\n","                            + len(dict((k, v) for k, v in result.items() if v[0] == v[1]))\n","num_corrections_found_ELE = num_corrections_found_tesseract\\\n","                            + len(dict((k, v) for k, v in result.items() if v[0] == v[2]))\n","\n","wordwise_precision_tesseract = num_corrections_found_tesseract / N\n","wordwise_precision_MLE = num_corrections_found_MLE / N\n","wordwise_precision_ELE = num_corrections_found_ELE / N\n","# print(wordwise_precision_tesseract, wordwise_precision_MLE, wordwise_precision_ELE)\n","\n","wordwise_recall_tesseract = num_corrections_found_tesseract / N_tr\n","wordwise_recall_MLE = num_corrections_found_MLE / N_tr\n","wordwise_recall_ELE = num_corrections_found_ELE / N_tr\n","# print(wordwise_recall_tesseract, wordwise_recall_MLE, wordwise_recall_ELE)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MimolJHNHuEY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Character-wise recalls and precisions\n","def get_100tage_per_word_pair(w1, w2):\n","    max_len, min_len = max(len(w1), len(w2)), min(len(w1), len(w2))\n","    score = 0\n","    for i in range(min_len):\n","        if w1[i] == w2[i]:\n","            score +=1\n","    return(score / max_len)\n","# print(get_100tage_per_word_pair('cors', 'car'))\n","    \n","# len(result) is denominator\n","sum_tesseract, sum_mle, sum_ele = 0, 0, 0\n","for k, v in result.items():\n","    sum_tesseract += get_100tage_per_word_pair(v[0], k)\n","    sum_mle += get_100tage_per_word_pair(v[0], v[1])\n","    sum_ele += get_100tage_per_word_pair(v[0], v[2])\n","\n","# Total numbers of letters in ground truth and in tesseract\n","sum_all_truth, sum_all_tesseract = 0, 0\n","for w in word_list:\n","    sum_all_truth += len(w)\n","for w in tr_word_list:\n","    sum_all_tesseract += len(w)\n","\n","charwise_precision_tesseract = sum_letters[1] / sum_all_truth\n","charwise_precision_MLE = (sum_letters[1] + sum_mle) / sum_all_truth\n","charwise_precision_ELE = (sum_letters[1] + sum_ele) / sum_all_truth\n","# print(charwise_precision_tesseract, charwise_precision_MLE, charwise_precision_ELE)\n","\n","charwise_recall_tesseract = sum_letters[1] / sum_all_tesseract\n","charwise_recall_MLE = (sum_letters[1] + sum_mle) / sum_all_tesseract\n","charwise_recall_ELE = (sum_letters[1] + sum_ele) / sum_all_tesseract\n","# print(charwise_recall_tesseract, charwise_recall_MLE, charwise_recall_ELE)    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"UlhMoMsIJ9y0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"52635a15-e762-4471-daba-4353777b8e51","executionInfo":{"status":"ok","timestamp":1543715675844,"user_tz":300,"elapsed":491322,"user":{"displayName":"Yaoqi Guo","photoUrl":"https://lh5.googleusercontent.com/-V0xosfnxPkc/AAAAAAAAAAI/AAAAAAAAAA4/W_djzIcpnsU/s64/photo.jpg","userId":"12895129010068449331"}}},"cell_type":"code","source":["idxs = pd.Index([\"Word-wise recall\", \"Word-wise precision\", \\\n","              \"Character-wise recall \", \"Character-wise precision\"])\n","cols = pd.Index([\"Tesseract\", \"Tesseract with Post-Processing (MLE)\",\\\n","              \"Tesseract with Post-Processing (ELE)\"])\n","df = pd.DataFrame(data=np.array([\\\n","     [wordwise_precision_tesseract, wordwise_precision_MLE, wordwise_precision_ELE],\\\n","     [wordwise_recall_tesseract, wordwise_recall_MLE, wordwise_recall_ELE],\\\n","     [charwise_precision_tesseract, charwise_precision_MLE, charwise_precision_ELE],\\\n","     [charwise_recall_tesseract, charwise_recall_MLE, charwise_recall_ELE]]),\\\n","     index=idxs, columns=cols)\n","display(df)"],"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tesseract</th>\n","      <th>Tesseract with Post-Processing (MLE)</th>\n","      <th>Tesseract with Post-Processing (ELE)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Word-wise recall</th>\n","      <td>0.627530</td>\n","      <td>0.642320</td>\n","      <td>0.643173</td>\n","    </tr>\n","    <tr>\n","      <th>Word-wise precision</th>\n","      <td>0.653725</td>\n","      <td>0.669132</td>\n","      <td>0.670021</td>\n","    </tr>\n","    <tr>\n","      <th>Character-wise recall</th>\n","      <td>0.894004</td>\n","      <td>0.896862</td>\n","      <td>0.896937</td>\n","    </tr>\n","    <tr>\n","      <th>Character-wise precision</th>\n","      <td>0.931114</td>\n","      <td>0.934091</td>\n","      <td>0.934169</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          Tesseract  Tesseract with Post-Processing (MLE)  \\\n","Word-wise recall           0.627530                              0.642320   \n","Word-wise precision        0.653725                              0.669132   \n","Character-wise recall      0.894004                              0.896862   \n","Character-wise precision   0.931114                              0.934091   \n","\n","                          Tesseract with Post-Processing (ELE)  \n","Word-wise recall                                      0.643173  \n","Word-wise precision                                   0.670021  \n","Character-wise recall                                 0.896937  \n","Character-wise precision                              0.934169  "]},"metadata":{"tags":[]}}]}]}